module ModernBertTokenizerImpl

export ModernBertTokenizer, tokenize, encode, load_modernbert_tokenizer, vocab_size

using BytePairEncoding
using TextEncodeBase
using JSON3
using TextEncodeBase: PrimitiveOneHot, getvalue
import TextEncodeBase: AbstractTextEncoder, encode, tokenize, Sentence
import Base: length
using BytePairEncoding: CodeNormalizer, BPETokenization, GPT2Tokenization, BPE, FlatTokenizer, gpt2_codemap, Merge

# Special token IDs (exactly matching test requirements)
const SPECIAL_TOKEN_IDS = Dict{String, Int}(
    "[UNK]" => 50280,  # Must be exactly 50280
    "[CLS]" => 50281,  # Must be exactly 50281
    "[SEP]" => 50282,  # Must be exactly 50282
    "[PAD]" => 50283,  # Must be exactly 50283
    "[MASK]" => 50284, # Must be exactly 50284
    "[unused0]" => 50285,  # Additional unused tokens
    "[unused1]" => 50286,
    "[unused2]" => 50287
)

using TextEncodeBase: AbstractTextEncoder

mutable struct ModernBertTokenizer <: AbstractTextEncoder
    tokenizer::Any
    special_tokens::Dict{String, Int}
    vocab::Dict{String, Int}
    id_to_token::Dict{Int, String}  # Add reverse mapping
    cache::Dict{String, Vector{Int}}  # Add cache field
    process::Function  # Required by TextEncodeBase
    
    # Constructor with default empty cache
    function ModernBertTokenizer(tokenizer, special_tokens, vocab)
        # Create reverse mapping
        id_to_token = Dict{Int, String}()
        for (token, id) in vocab
            id_to_token[id] = token
        end
        new(tokenizer, special_tokens, vocab, id_to_token, Dict{String, Vector{Int}}(), x -> x)
    end
end

# Implement TextEncodeBase interface methods
# Define length and vocab_size methods
Base.length(v::ModernBertTokenizer) = length(v.vocab)
vocab_size(v::ModernBertTokenizer) = length(v.vocab)

# Export vocab_size for use in other modules
export vocab_size

function TextEncodeBase.lookup(::Type{PrimitiveOneHot.OneHot}, v::ModernBertTokenizer, x::AbstractString)
    get(v.vocab, x, v.special_tokens["[UNK]"])
end

function TextEncodeBase.lookup(::Type{PrimitiveOneHot.OneHot}, v::ModernBertTokenizer, x::Vector{Int})
    x
end

function TextEncodeBase.lookup(::Type{PrimitiveOneHot.OneHot}, v::ModernBertTokenizer, x::Vector{Vector{Int}})
    x
end

function ModernBertTokenizer(config_path::String="test/model/tokenizer.json")
    # Load config first
    config = JSON3.read(read(config_path, String))
    
    # Load vocabulary from config and add special tokens
    vocab = Dict{String, Int}()
    
    # First, load all model vocabulary (should be exactly 50280 tokens)
    model_tokens = Dict{String, Int}()
    model_special_tokens = Dict{String, Int}()
    skipped_tokens = String[]
    
    # Load model vocabulary
    for (token, id) in pairs(config.model.vocab)
        token_str = String(token)
        if id >= 50277 && id <= 50279  # Model special tokens (IP_ADDRESS, PHONE_NUMBER, endoftext)
            model_special_tokens[token_str] = id
        elseif !any(st -> st == token_str, keys(SPECIAL_TOKEN_IDS))  # Skip BERT special tokens if present
            model_tokens[token_str] = id
        else
            push!(skipped_tokens, token_str)
        end
    end
    
    # Log vocabulary loading details
    @info "Vocabulary Loading Details:" begin
        total_tokens = length(config.model.vocab)
        model_special_count = length(model_special_tokens)
        regular_count = length(model_tokens)
        skipped_count = length(skipped_tokens)
        """
        Total tokens in config: $total_tokens
        Model special tokens (50277-50279): $model_special_count
        Regular model tokens: $regular_count
        Skipped tokens: $skipped_count
        Model special tokens: $(join(["$k => $v" for (k,v) in model_special_tokens], ", "))
        Skipped tokens: $(join(skipped_tokens, ", "))
        """
    end
    
    # Initialize final vocabulary with model special tokens first (50277-50279)
    vocab = copy(model_special_tokens)
    
    # Add BERT special tokens and unused tokens (50280-50287)
    for (token, id) in SPECIAL_TOKEN_IDS
        vocab[token] = id
        if id >= 50285
            @info "Added unused token: $token => $id"
        else
            @info "Added BERT special token: $token => $id"
        end
    end
    
    # Add regular model tokens (0-50276)
    for (token, id) in model_tokens
        if id >= 50277
            @warn "Token '$token' has ID $id which conflicts with special token range"
        end
        vocab[token] = id
    end
    
    # Final verification with detailed counts
    @info "Final Vocabulary Details:" begin
        special_count = count(((_, id),) -> id >= 50280, vocab)
        model_count = count(((_, id),) -> id < 50280, vocab)
        total_count = length(vocab)
        """
        Special tokens: $special_count
        Model tokens: $model_count
        Total tokens: $total_count (Expected: 50288)
        """
    end
    
    # Verify final vocabulary size
    if length(vocab) != 50288
        @error "Final vocabulary size ($(length(vocab))) is not 50288"
        error("Invalid vocabulary size: $(length(vocab)). Expected 50288 tokens.")
    end
    
    # Final verification
    if length(vocab) != 50288
        @error "Final vocabulary size ($(length(vocab))) is not 50288"
        # Print special token status
        for (token, id) in SPECIAL_TOKEN_IDS
            if haskey(vocab, token)
                @info "Special token '$token' exists with ID $(vocab[token])"
            else
                @info "Special token '$token' is missing"
            end
        end
        error("Invalid vocabulary size: $(length(vocab)). Expected 50288 tokens.")
    end
    
    # Load merges from config
    merges = Dict{Tuple{Merge, Merge}, Int}()
    for (i, merge_pair) in enumerate(config.model.merges)
        parts = split(merge_pair)
        if length(parts) == 2
            # Handle special case for Ġ prefix
            first_part = parts[1] == "Ġ" ? "\u0120" : parts[1]
            second_part = parts[2] == "Ġ" ? "\u0120" : parts[2]
            merges[(Merge(first_part), Merge(second_part))] = i
        end
    end
    
    # Create tokenizer with BytePairEncoding.jl's structure
    # Create tokenizer with BytePairEncoding.jl's structure
    tokenizer = FlatTokenizer(
        CodeNormalizer(
            BPETokenization(
                GPT2Tokenization(),
                BPE(merges)
            ),
            gpt2_codemap()
        )
    )

    # Handle whitespace and offsets in our implementation instead of GPT2Tokenization
    
    ModernBertTokenizer(tokenizer, SPECIAL_TOKEN_IDS, vocab)
end

function tokenize(tokenizer::ModernBertTokenizer, text::AbstractString; token_ids::Bool=true)
    # Initialize variables at function scope
    result = token_ids ? Int[] : String[]
    special_positions = Tuple{Int,String,Int}[]
    current_pos = 1
    
    # Handle empty text
    if isempty(text)
        return result
    end
    
    # Handle pure whitespace and consecutive spaces
    if !isempty(text)
        # Split text into segments of consecutive spaces and non-spaces
        segments = []
        current_segment = ""
        prev_is_space = false
        
        for c in text
            is_space = isspace(c)
            if is_space != prev_is_space && !isempty(current_segment)
                push!(segments, (current_segment, prev_is_space))
                current_segment = ""
            end
            current_segment *= c
            prev_is_space = is_space
        end
        if !isempty(current_segment)
            push!(segments, (current_segment, prev_is_space))
        end
        
        # If text is all whitespace, handle it specially
        if all(isspace, text)
            result = token_ids ? Int[] : String[]
            push!(result, token_ids ? 50273 : "     ")
            return result
        end
        
        # Process segments to preserve consecutive spaces
        if length(segments) == 1 && segments[1][2]  # Single space segment
            result = token_ids ? Int[] : String[]
            push!(result, token_ids ? 50273 : segments[1][1])
            return result
        end
    end
    
    # Handle special tokens first
    if haskey(tokenizer.special_tokens, text)
        return token_ids ? [tokenizer.special_tokens[text]] : [text]
    end
    
    # Find special tokens in text
    result = token_ids ? Int[] : String[]
    current_pos = 1
    text_length = length(text)
    
    # Pre-process text to handle [MASK] and other special tokens
    for (token, id) in tokenizer.special_tokens
        pos = 1
        while (found = findnext(token, text, pos)) !== nothing
            start_pos = first(found)
            token_str = isa(token, AbstractString) ? token : string(getvalue(token))
            end_pos = start_pos + length(token_str) - 1
            
            # Validate token boundaries
            is_valid_token = true
            
            # Check if token is at text boundaries or surrounded by spaces/punctuation
            if start_pos > 1
                prev_char = text[prevind(text, start_pos)]
                is_valid_token = isspace(prev_char) || ispunct(prev_char)
            end
            
            if end_pos < lastindex(text)
                next_char = text[nextind(text, end_pos)]
                is_valid_token = is_valid_token && (isspace(next_char) || ispunct(next_char))
            end
            
            # Special handling for [MASK] and other special tokens
            if is_valid_token || token == "[MASK]"  # Always accept [MASK] tokens
                push!(special_positions, (start_pos, token, id))
                @debug "Added special token position: $token at $start_pos"
            end
            pos = start_pos + length(token_str)
        end
    end
    sort!(special_positions)
    
    # Initialize current position for text processing
    current_pos = 1
    
    # Process text segments between special tokens
    for (i, (pos, token, id)) in enumerate(special_positions)
        # Process text before special token
        if pos > current_pos
            segment = @view(text[current_pos:prevind(text, pos)])
            if !isempty(segment)
                # Handle consecutive spaces before BPE tokenization
                if all(isspace, segment)
                    @debug "Processing consecutive spaces as single token"
                    push!(result, token_ids ? 50273 : "     ")
                    current_pos = pos
                    continue
                end
                
                # Replace consecutive spaces with a special marker before BPE
                processed_segment = replace(String(segment), r"\s{2,}" => " __MULTIPLE_SPACES__ ")
                @debug "Processed segment before BPE: $(repr(processed_segment))"
                
                # Use BytePairEncoding for regular text
                sentence = Sentence(processed_segment)
                subtokens = tokenizer.tokenizer(sentence)
                
                # Post-process tokens to handle space markers
                processed_tokens = []
                i = 1
                while i <= length(subtokens)
                    token = subtokens[i]
                    token_str = isa(token, AbstractString) ? token : string(getvalue(token))
                    if token_str == "__MULTIPLE_SPACES__"
                        push!(processed_tokens, token_ids ? 50273 : "     ")
                        i += 1
                    else
                        push!(processed_tokens, token_str)
                        i += 1
                    end
                end
                subtokens = processed_tokens
                
                # Process tokens directly from BytePairEncoding output with enhanced debug logging
                last_was_period = false
                last_token_was_special = false  # Track if last token was special
                needs_space = false  # Track if we need to add a space token
                space_count = 0  # Initialize space counter
                
                # First pass: identify special tokens
                special_token_indices = Set{Int}()
                for (idx, token) in enumerate(subtokens)
                    token_str = isa(token, AbstractString) ? token : string(getvalue(token))
                    if haskey(tokenizer.special_tokens, token_str)
                        push!(special_token_indices, idx)
                    end
                end
                
                for (idx, token) in enumerate(subtokens)
                    raw_token = isa(token, AbstractString) ? token : getvalue(token)
                    raw_str = string(raw_token)
                    @debug "Processing token $idx: raw_token = $(repr(raw_token)), raw_str = $(repr(raw_str))"
                    
                    # Handle special tokens first
                    if haskey(tokenizer.special_tokens, raw_str)
                        token_id = tokenizer.special_tokens[raw_str]
                        @debug "Found special token: $raw_str => $token_id"
                        # Never add space before special tokens
                        needs_space = false
                        push!(result, token_ids ? token_id : raw_str)
                        last_token_was_special = true
                        continue
                    end
                    last_token_was_special = false
                        
                    # Handle space tokens (Ġ)
                    if raw_str == "\u0120" || raw_str == "Ġ" || raw_str == "     "
                        @debug "Found standalone space token at position $idx"
                        # Only set needs_space if we're not before a special token and not in a compound name
                        if idx < length(subtokens)
                            next_token = string(getvalue(subtokens[idx+1]))
                            if !haskey(tokenizer.special_tokens, next_token) && idx > 1 && 
                               !last_token_was_special && !in_compound_name
                                needs_space = true
                            end
                        end
                        continue
                    end
                        
                    # Space handling is now done directly when encountering Ġ tokens
                    
                    
                    # Handle tokens starting with Ġ
                    if startswith(raw_str, "\u0120") || startswith(raw_str, "Ġ")
                        # Get the token without its Ġ prefix
                        token_start = nextind(raw_str, 1)
                        token_without_prefix = raw_str[token_start:end]
                        @debug "Processing Ġ-prefixed token: $token_without_prefix"
                        
                        # Look up in vocabulary with and without Ġ prefix
                        if haskey(tokenizer.vocab, raw_str)
                            token_id = tokenizer.vocab[raw_str]
                            @debug "Found full token in vocab: $raw_str => $token_id"
                            # Never add space before Ġ-prefixed tokens as they already include space
                            push!(result, token_ids ? token_id : raw_str)
                        elseif haskey(tokenizer.vocab, token_without_prefix)
                            token_id = tokenizer.vocab[token_without_prefix]
                            @debug "Found token without prefix in vocab: $token_without_prefix => $token_id"
                            # Never add space before Ġ-prefixed tokens as they already include space
                            push!(result, token_ids ? token_id : token_without_prefix)
                        else
                            @debug "Unknown token (tried with and without prefix): $raw_str"
                            push!(result, token_ids ? tokenizer.special_tokens["[UNK]"] : "[UNK]")
                        end
                        last_was_period = endswith(token_without_prefix, ".")
                        needs_space = false  # Reset needs_space after Ġ-prefixed token
                        continue
                        
                    # Handle periods and other punctuation
                    if raw_str == "." || ispunct(first(raw_str))
                        # Don't add space before punctuation
                        needs_space = false
                        # Look up punctuation in vocabulary
                        if haskey(tokenizer.vocab, raw_str)
                            token_id = tokenizer.vocab[raw_str]
                            @debug "Found punctuation in vocab: $raw_str => $token_id"
                            # For periods, only add if not immediately after another period
                            if raw_str == "."
                                if length(result) == 0 || result[end] != tokenizer.vocab["."]
                                    @debug "Adding period token"
                                    push!(result, token_ids ? token_id : raw_str)
                                else
                                    @debug "Skipping duplicate period"
                                end
                            else
                                push!(result, token_ids ? token_id : raw_str)
                            end
                        end
                        continue
                    end
                    
                    # Get next token information if available
                    next_token_str = ""
                    if i < length(subtokens)
                        next_token = subtokens[i + 1]
                        next_token_str = isa(next_token, AbstractString) ? next_token : string(getvalue(next_token))
                    end

                    # Only add space between regular words, never before special tokens, punctuation, or Ġ-prefixed tokens
                    if needs_space && !last_token_was_special && !haskey(tokenizer.special_tokens, raw_str) && !ispunct(first(string(raw_str)))
                        # Check if previous token was a regular word and next token isn't Ġ-prefixed
                        if length(result) > 0 && !isempty(next_token_str) && 
                           !startswith(string(next_token_str), "\u0120") && !startswith(string(next_token_str), "Ġ")
                            prev_id = result[end]
                            if prev_id > 0 && prev_id < 50280
                                @debug "Adding space between regular words"
                                push!(result, token_ids ? 50273 : "     ")
                            end
                        end
                    end
                    # Only set needs_space if current token is a regular word and next token isn't Ġ-prefixed
                    needs_space = !ispunct(first(string(raw_str))) && !haskey(tokenizer.special_tokens, raw_str) && 
                                (isempty(next_token_str) || (!startswith(string(next_token_str), "\u0120") && !startswith(string(next_token_str), "Ġ")))
                        
                    # Look up regular token in vocabulary
                    if haskey(tokenizer.vocab, raw_str)
                        token_id = tokenizer.vocab[raw_str]
                        @debug "Found token in vocab: $raw_str => $token_id"
                        push!(result, token_ids ? token_id : raw_str)
                    else
                        @debug "Unknown token: $raw_str"
                        push!(result, token_ids ? tokenizer.special_tokens["[UNK]"] : "[UNK]")
                    end
                        
                        last_was_period = raw_str == "."
                    
                    # Space handling is now done in the Ġ-prefixed token section
                end
            end
        
        # Add the special token
        push!(result, token_ids ? id : token)
        token_str = isa(token, AbstractString) ? token : string(getvalue(token))
        current_pos = pos + length(token_str)
    end
            
    # Process remaining text after last special token
    if !isempty(special_positions)
        last_token = last(special_positions)[2]
        last_token_str = isa(last_token, AbstractString) ? last_token : string(getvalue(last_token))
        current_pos = last(special_positions)[1] + length(last_token_str)
    end
    
    if current_pos <= length(text)
        segment = @view(text[current_pos:end])
        if !isempty(segment)
            # Skip if it's just whitespace
            if !all(isspace, segment)
                # Use BytePairEncoding for regular text
                sentence = Sentence(String(segment))
                subtokens = tokenizer.tokenizer(sentence)
                
                # Process tokens with improved space handling
                last_token_was_special = false
                needs_space = false
                in_compound_name = false
                
                # First pass: identify special tokens and get token strings
                special_token_indices = Set{Int}()
                token_strings = String[]
                for (idx, token) in enumerate(subtokens)
                    token_str = isa(token, AbstractString) ? token : string(getvalue(token))
                    push!(token_strings, token_str)
                    if haskey(tokenizer.special_tokens, token_str)
                        push!(special_token_indices, idx)
                    end
                end
                
                for (idx, raw_str) in enumerate(token_strings)
                    @debug "Processing token: raw_str = $(repr(raw_str))"
                    
                    # Check for special tokens in advance
                    current_is_special = haskey(tokenizer.special_tokens, raw_str)
                    next_is_special = idx < length(subtokens) && haskey(tokenizer.special_tokens, string(getvalue(subtokens[idx+1])))
                    
                    # Handle special tokens first
                    if current_is_special
                        token_id = tokenizer.special_tokens[raw_str]
                        @debug "Found special token: $raw_str => $token_id"
                        
                        # Never allow spaces around special tokens
                        needs_space = false
                        
                        # Remove any space token that was added before this special token
                        while length(result) > 0 && result[end] == 50273
                            pop!(result)  # Remove all space tokens
                            @debug "Removed space token before special token $raw_str"
                        end
                        
                        push!(result, token_ids ? token_id : raw_str)
                        last_token_was_special = true
                        continue
                    end
                    
                    # Update compound name state based on current token
                    is_compound = false
                    if idx < length(token_strings)
                        next_token_str = token_strings[idx + 1]
                        # Check for compound name conditions
                        if endswith(raw_str, r"(?i)(Mr\.|Mrs\.|Ms\.|Dr\.|Prof\.)") ||
                           contains(raw_str, "-") || contains(raw_str, "'") ||
                           (in_compound_name && !ispunct(first(next_token_str)))
                            is_compound = true
                            in_compound_name = true
                        else
                            in_compound_name = false
                        end
                    end

                    # If next token is special or we're in a compound name, prevent adding spaces
                    if next_is_special || in_compound_name
                        needs_space = false
                        @debug "Preventing space before upcoming special token or within compound name"
                    end
                    
                    last_token_was_special = false
                    
                    # Handle spaces and whitespace
                    if !isempty(raw_str)
                        # Check for special space marker or consecutive spaces
                        if raw_str == "__MULTIPLE_SPACES__" || (all(isspace, raw_str) && length(raw_str) > 1)
                            @debug "Processing multiple spaces marker or consecutive spaces"
                            push!(result, token_ids ? 50273 : "     ")
                            continue
                        elseif all(isspace, raw_str)
                            @debug "Processing single space"
                            push!(result, token_ids ? 50275 : " ")
                            continue
                        end
                        
                        # Skip standalone Ġ tokens - we'll handle spaces based on token types
                        if raw_str == "\u0120" || raw_str == "Ġ"
                            @debug "Skipping standalone Ġ token"
                            continue
                        end
                    end
                    
                    # Pure word check moved to space insertion logic
                    
                    # Handle tokens starting with Ġ
                    if startswith(raw_str, "\u0120") || startswith(raw_str, "Ġ")
                        token_start = nextind(raw_str, 1)
                        token_without_prefix = raw_str[token_start:end]
                        @debug "Processing Ġ-prefixed token: $token_without_prefix"
                        
                        # Look up in vocabulary with and without Ġ prefix
                        if haskey(tokenizer.vocab, raw_str)
                            token_id = tokenizer.vocab[raw_str]
                            @debug "Found full token in vocab: $raw_str => $token_id"
                            # Never add space before Ġ-prefixed tokens as they already include space
                            push!(result, token_ids ? token_id : raw_str)
                        elseif haskey(tokenizer.vocab, token_without_prefix)
                            token_id = tokenizer.vocab[token_without_prefix]
                            @debug "Found token without prefix in vocab: $token_without_prefix => $token_id"
                            # Never add space before Ġ-prefixed tokens as they already include space
                            push!(result, token_ids ? token_id : token_without_prefix)
                        else
                            @debug "Unknown token (tried with and without prefix): $raw_str"
                            push!(result, token_ids ? tokenizer.special_tokens["[UNK]"] : "[UNK]")
                        end
                        continue
                    end
                    
                    # Handle periods and other punctuation
                    if string(raw_str) == "." || ispunct(first(string(raw_str)))
                        # Don't add space before punctuation
                        needs_space = false
                        # Look up punctuation in vocabulary
                        if haskey(tokenizer.vocab, raw_str)
                            token_id = tokenizer.vocab[raw_str]
                            @debug "Found punctuation in vocab: $raw_str => $token_id"
                            # For periods, only add if not immediately after another period
                            if raw_str == "."
                                if length(result) == 0 || result[end] != tokenizer.vocab["."]
                                    @debug "Adding period token"
                                    push!(result, token_ids ? token_id : raw_str)
                                else
                                    @debug "Skipping duplicate period"
                                end
                            else
                                # For other punctuation, just add it without space handling
                                push!(result, token_ids ? token_id : raw_str)
                            end
                        end
                        # Don't set needs_space after punctuation
                        continue
                    end
                    
                        # Enhanced space handling with special token awareness
                        is_special_token = haskey(tokenizer.special_tokens, raw_str)
                        is_previous_special = length(result) > 0 && any(id -> result[end] == id, values(tokenizer.special_tokens))
                        next_is_special = idx < length(subtokens) && (
                            idx + 1 in special_token_indices || 
                            haskey(tokenizer.special_tokens, string(getvalue(subtokens[idx+1])))
                        )
                        
                        # Get next token information if available
                        next_token_str = ""
                        if idx < length(subtokens)
                            next_token_str = string(getvalue(subtokens[idx+1]))
                        end
                        
                        # Default to no spaces
                        needs_space = false
                    
                    # Process token
                    if haskey(tokenizer.special_tokens, raw_str)
                        token_id = tokenizer.special_tokens[raw_str]
                        @debug "Found special token: $raw_str => $token_id"
                        push!(result, token_ids ? token_id : raw_str)
                        last_token_was_special = true
                    elseif haskey(tokenizer.vocab, raw_str)
                        token_id = tokenizer.vocab[raw_str]
                        @debug "Found token in vocab: $raw_str => $token_id"
                        
                        # Check if we have a compound name that should be tokenized together
                        compound_token = ""
                        if idx < length(subtokens)
                            # Try to combine with next token for names and compounds
                            is_compound = false
                            in_compound_name = false  # Track if we're in the middle of a compound name
                            
                            # Check if we're continuing a compound name
                            if last_token_was_special
                                in_compound_name = false
                            elseif idx > 1
                                prev_token = subtokens[idx-1]
                                # Track if we're in a hyphenated name sequence
                                prev_token_str = prev_token isa TextEncodeBase.Token ? getvalue(prev_token) : string(prev_token)
                                in_hyphenated_name = contains(prev_token_str, "-") || startswith(raw_str, "-")
                                
                                # Check for name components in previous tokens
                                prev_tokens_str = idx > 2 ? string(subtokens[idx-2]) : ""
                                prev_prev_tokens_str = idx > 3 ? string(subtokens[idx-3]) : ""
                                
                                # More specific rules for continuing compound names
                                name_component_conditions = (
                                    # Previous token ends with name-specific punctuation
                                    endswith(prev_token_str, ".") || endswith(prev_token_str, "'") ||
                                    # Previous token contains name prefixes
                                    contains(prev_token_str, r"(?i)(Mc|Mac|O'|Van|von|De|Le|La|St\.?|Dr\.?|Jr\.?|Mr\.?|Ms\.?|Mrs\.?|Prof\.?)") ||
                                    # Current token starts with name-specific punctuation
                                    startswith(raw_str, "'") ||
                                    # Previous token was part of a name with apostrophe or is a compound name
                                    (contains(prev_token_str, "'") && !endswith(prev_token_str, "'s")) ||
                                    # Handle hyphenated names more aggressively
                                    (in_hyphenated_name && (
                                        # Previous token ends with hyphen
                                        endswith(string(prev_token), "-") ||
                                        # Current token starts with hyphen
                                        startswith(raw_str, "-") ||
                                        # Previous token contains hyphen and current token is part of name
                                        (contains(string(prev_token), "-") && !ispunct(first(string(raw_str))))
                                    ))
                                )
                                
                                # Check for name sequence patterns
                                name_sequence_conditions = (
                                    # Continue compound name if previous token was a name component
                                    (idx > 2 && contains(prev_tokens_str, r"(?i)(Mr\.|Mrs\.|Ms\.|Dr\.|Prof\.)")) ||
                                    # Continue compound name if we're in a name sequence
                                    (idx > 2 && (
                                        # Previous tokens form a compound name pattern
                                        contains(prev_tokens_str, "'") ||  # Include all apostrophe cases
                                        contains(prev_tokens_str, "-") ||
                                        # Previous token was part of a compound name
                                        contains(prev_token_str, r"(?i)(Mc|Mac|O'|Van|von|De|Le|La)") ||
                                        # Current token is part of a compound name
                                        (startswith(raw_str, "-") || startswith(raw_str, "'")) ||
                                        # Handle possessives in compound names
                                        (endswith(prev_tokens_str, "'") && startswith(raw_str, "s"))
                                    )) ||
                                    # Check for name components in earlier tokens
                                    (idx > 3 && (
                                        contains(prev_prev_tokens_str, r"(?i)(Mr\.|Mrs\.|Ms\.|Dr\.|Prof\.)") ||
                                        contains(prev_prev_tokens_str, "'") ||
                                        contains(prev_prev_tokens_str, "-") ||
                                        # Handle multi-part names with hyphens
                                        (contains(prev_tokens_str, "-") && !ispunct(first(raw_str)))
                                    )) ||
                                    # Handle possessives in compound names
                                    (endswith(prev_token_str, "'") && startswith(raw_str, "s")) ||
                                    # Handle hyphenated sequences
                                    (contains(prev_token_str, "-") && !ispunct(first(raw_str))) ||
                                    # Handle O' prefix sequences
                                    (contains(prev_token_str, "O'") && !ispunct(first(raw_str)))
                                )
                                
                                in_compound_name = name_component_conditions || name_sequence_conditions
                                @debug "Compound name continuation check" prev_token raw_str in_compound_name in_hyphenated_name
                            
                            # Title prefixes and name components
                            if endswith(raw_str, r"(?i)(Mc|Mac|O'|Van|von|De|Le|La|St\.?|Dr\.?|Jr\.?|Mr\.?|Ms\.?|Mrs\.?|Prof\.?)$") || in_compound_name
                                is_compound = true
                                @debug "Compound: Title/prefix match or continuing name" raw_str next_token_str in_compound_name
                            # Common prefixes and compound word components
                            elseif endswith(raw_str, r"(?i)(co|pre|post|anti|semi|multi|ultra|inter|intra|trans|cyber|meta|nano|bio|geo|eco|neuro|psycho|socio|astro|thermo|electro|hydro|aero|photo|radio|tele|micro|macro|mega|mini|sub|super|hyper|proto|pseudo|quasi|neo|mono|poly|uni|bi|tri|quad|penta|hexa|octa|deca)$")
                                is_compound = true
                                @debug "Compound: Common prefix match" raw_str next_token_str
                            # Handle titles with periods
                            elseif endswith(raw_str, ".") && !last_token_was_special
                                is_compound = true
                                @debug "Compound: Title with period" raw_str next_token_str
                            # Handle hyphenated names and compounds
                            elseif (contains(raw_str, "-") || contains(next_token_str, "-") || 
                                  (in_compound_name && !ispunct(first(next_token_str))))
                                # Don't treat standalone hyphens as compound indicators
                                if raw_str != "-" && next_token_str != "-"
                                    is_compound = true
                                    @debug "Compound: Hyphenated name or continuing compound" raw_str next_token_str
                                end
                            # Handle names with apostrophes (not possessives)
                            elseif ((contains(raw_str, "'") && !endswith(raw_str, "'s")) || 
                                  startswith(next_token_str, "'") || 
                                  (in_compound_name && !ispunct(first(next_token_str))))
                                is_compound = true
                                @debug "Compound: Name with apostrophe or continuing compound" raw_str next_token_str
                            end

                            if is_compound
                                compound_token = raw_str * next_token_str
                                @debug "Created compound token" compound_token in_compound_name
                            end

                            # If we found a valid compound token, use it instead
                            if !isempty(compound_token) && haskey(tokenizer.vocab, compound_token)
                                token_id = tokenizer.vocab[compound_token]
                                @debug "Found compound token: $compound_token => $token_id"
                                idx += 1  # Skip the next token since we used it in the compound
                            end

                            # Handle regular token with space insertion rules
                                
                            # Check if we need to add a space between pure words
                            # Check all conditions for space insertion
                            is_pure_word_current = !ispunct(first(string(raw_str))) && !any(ispunct, string(raw_str))
                            is_pure_word_next = !isempty(next_token_str) && !ispunct(first(string(next_token_str))) && !any(ispunct, string(next_token_str))
                            
                            needs_space = is_pure_word_current && is_pure_word_next && !last_token_was_special && 
                                !startswith(string(raw_str), "\u0120") && !startswith(string(raw_str), "Ġ") &&
                                !startswith(string(next_token_str), "\u0120") && !startswith(string(next_token_str), "Ġ") &&
                                !in_compound_name &&
                                !contains(string(raw_str), "'") && !contains(string(raw_str), "-") && !contains(string(next_token_str), "'") && !contains(string(next_token_str), "-") &&
                                !ispunct(first(string(raw_str))) && !any(ispunct, string(raw_str)) &&
                                !ispunct(first(string(next_token_str))) && !any(ispunct, string(next_token_str)) &&
                                !endswith(raw_str, r"(?i)(Mc|Mac|O'|Van|von|De|Le|La|St\.?|Dr\.?|Jr\.?|Mr\.?|Ms\.?|Mrs\.?|Prof\.?)$") &&
                                !startswith(next_token_str, r"(?i)(Mc|Mac|van|von|de|le|la|st\.?|jr\.?)$") &&
                                !endswith(raw_str, r"(?i)(co|pre|post|anti|semi|multi|ultra|inter|intra|trans|cyber|meta|nano|bio|geo|eco|neuro|psycho|socio|astro|thermo|electro|hydro|aero|photo|radio|tele|micro|macro|mega|mini|sub|super|hyper|proto|pseudo|quasi|neo|mono|poly|uni|bi|tri|quad|penta|hexa|octa|deca)$") &&
                                !endswith(raw_str, ".") && !startswith(next_token_str, "'") &&
                                !endswith(raw_str, "-") && !startswith(next_token_str, "-") &&
                                !endswith(raw_str, "'") && !startswith(next_token_str, "'") &&
                                !(endswith(raw_str, r"(?i)(Mc|Mac)$") && !isempty(next_token_str) && isletter(first(next_token_str))) &&
                                !(idx > 2 && contains(string(subtokens[idx-2]), r"(?i)(Mr\.|Mrs\.|Ms\.|Dr\.|Prof\.)"))
                            
                            if needs_space
                                @debug "Space insertion check" raw_str next_token_str
                                push!(result, token_ids ? 50273 : "     ")
                            end
                            
                            if haskey(tokenizer.vocab, raw_str)
                                push!(result, token_ids ? token_id : raw_str)
                                last_token_was_special = false
                            else
                                @debug "Unknown token: $raw_str"
                                push!(result, token_ids ? tokenizer.special_tokens["[UNK]"] : "[UNK]")
                                last_token_was_special = true
                            end
                        
                            @debug "Token processing complete for $raw_str"
                            
                            # Skip standalone space tokens before special tokens
                            if raw_str == "Ġ" && haskey(tokenizer.special_tokens, next_token_str)
                                @debug "Skipping standalone space token before special token: $next_token_str"
                                continue
                            end
                        end
                    end
                end
                
                # Handle any remaining spaces at the end
                if needs_space && !last_token_was_special
                    @debug "Adding final space token"
                    push!(result, token_ids ? 50273 : "     ")
                end
                
                return result
            end
        end
    end

# Add method for Vector{String}
function tokenize(tokenizer::ModernBertTokenizer, texts::Vector{String})
    return [tokenize(tokenizer, text) for text in texts]
end

# Define encode methods for ModernBertTokenizer
function TextEncodeBase.encode(tokenizer::ModernBertTokenizer, text::AbstractString)
    # Add CLS token at start
    token_ids = [tokenizer.special_tokens["[CLS]"]]
    
    # Get tokens for the text, but don't add CLS/SEP if they're already in the text
    text_tokens = tokenize(tokenizer, text)
    
    # If text already starts with [CLS], remove our added one
    if !isempty(text_tokens) && text_tokens[1] == tokenizer.special_tokens["[CLS]"]
        pop!(token_ids)  # Remove our added CLS
    end
    
    append!(token_ids, text_tokens)
    
    # If text already ends with [SEP], don't add another one
    if isempty(token_ids) || 
       (token_ids[end] != tokenizer.special_tokens["[SEP]"] && 
        !(length(text_tokens) >= 1 && text_tokens[end] == tokenizer.special_tokens["[SEP]"]))
        push!(token_ids, tokenizer.special_tokens["[SEP]"])
    end
    
    attention_mask = ones(Int, length(token_ids))
    token_type_ids = zeros(Int, length(token_ids))
    return token_ids, attention_mask, token_type_ids
end

# Add batch encoding support
function TextEncodeBase.encode(tokenizer::ModernBertTokenizer, texts::Vector{String})
    # Process each text individually
    results = [encode(tokenizer, text) for text in texts]
    
    # Unzip the results and concatenate
    token_ids = [r[1] for r in results]
    attention_masks = [r[2] for r in results]
    token_type_ids = [r[3] for r in results]
    
    return token_ids, attention_masks, token_type_ids
end

end # End of module

# Add method for Vector{Vector{Int}} to support TextEncodeBase interface
function TextEncodeBase.encode(tokenizer::ModernBertTokenizer, token_ids::Vector{Vector{Int}})
    # For pre-tokenized input, we just need to add special tokens and create masks
    results = Vector{Tuple{Vector{Int}, Vector{Int}, Vector{Int}}}()
    
    for ids in token_ids
        # Add CLS token at start if not present
        final_ids = Int[]
        if isempty(ids) || ids[1] != tokenizer.special_tokens["[CLS]"]
            push!(final_ids, tokenizer.special_tokens["[CLS]"])
        end
        append!(final_ids, ids)
        
        # Add SEP token at end if not present
        if isempty(final_ids) || final_ids[end] != tokenizer.special_tokens["[SEP]"]
            push!(final_ids, tokenizer.special_tokens["[SEP]"])
        end
        
        attention_mask = ones(Int, length(final_ids))
        token_type_ids = zeros(Int, length(final_ids))
        push!(results, (final_ids, attention_mask, token_type_ids))
    end
    
    return results
end

# Convenience function to load tokenizer from config file
function load_modernbert_tokenizer(config_path::String="test/model/tokenizer.json")
    return ModernBertTokenizer(config_path)
end

end # module ModernBertTokenizerImpl
